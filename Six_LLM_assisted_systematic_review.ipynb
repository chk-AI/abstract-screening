{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chk-AI/abstract-screening/blob/main/Six_LLM_assisted_systematic_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 1: Installing necessary libraries and save secrets (API-keys and e-mail)"
      ],
      "metadata": {
        "id": "GzN-8O2561Cc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qErBXpYfxEcU"
      },
      "outputs": [],
      "source": [
        "!pip install biopython\n",
        "!pip install pandas\n",
        "!pip install -q openai\n",
        "!pip install anthropic\n",
        "!pip install openpyxl\n",
        "\n",
        "from google.colab import userdata\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "import json\n",
        "from Bio import Entrez\n",
        "from openai import OpenAI\n",
        "import anthropic\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save API keys for OpenAI and Anthopic and email adress for PubMed-API in sectrets (key in the left column).\n",
        "#API keys should be kept secret.\n",
        "\n",
        "openai_api_secret_name = 'OPENAI_API_KEY'\n",
        "# Try-except blocks for handling secret retrieval and initialization\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get(openai_api_secret_name)\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"There was an error initializing OpenAI client with the API key: {e}\")\n",
        "    raise e\n",
        "\n",
        "# Anthropics API key\n",
        "anthropic_api_secret_name = 'CLAUDE_API_KEY'\n",
        "CLAUDE_API_KEY = userdata.get(anthropic_api_secret_name)\n",
        "anthropic_client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
        "\n",
        "# Email for Biopython\n",
        "Entrez.email = userdata.get('USER_EMAIL')"
      ],
      "metadata": {
        "id": "RYJ7cDWLeaN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 2: Pubmed search"
      ],
      "metadata": {
        "id": "HH4pzEWSk87f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FhYi6o8xVMP"
      },
      "outputs": [],
      "source": [
        "# PubMed search terms and combined search string\n",
        "# Modify this search string for your systematic review\n",
        "\n",
        "search_string_1 = '(\"Central Nervous System\"[Mesh]) OR (brain*[All Fields]) OR (cerebr*[All Fields])'\n",
        "search_string_2 = '(\"Diagnostic Imaging\"[Mesh] OR CT[All Fields]) OR (MRI[All Fields])'\n",
        "search_string_3 = '(\"Deep Learning\"[Mesh]) OR (\"Neural Networks, Computer\"[Mesh]) OR (Neural network*[All Fields]) OR (Convolutional network*[All Fields]) OR (Deep learn*[All Fields]) OR (Artificial Intelligence*[All Fields])'\n",
        "\n",
        "# Including the date range from January 1, 2017, to April 19, 2024\n",
        "date_range = '(\"2017/01/01\"[PDAT] : \"2024/04/19\"[PDAT])'\n",
        "reviews = '(review[Publication Type])'\n",
        "\n",
        "# Combined search string with filters\n",
        "combined_search_string = f\"({search_string_1}) AND ({search_string_2}) AND ({search_string_3}) AND ({date_range}) NOT ({reviews})\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5gSeUeLxdQH"
      },
      "outputs": [],
      "source": [
        "#Functions for extracting search results and variables from studies from PubMed\n",
        "def search_pubmed(query, retmax=10000):\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=retmax)\n",
        "    record = Entrez.read(handle, validate=False)\n",
        "    handle.close()\n",
        "    time.sleep(0.4)  # To prevent overwhelming the server\n",
        "    return record[\"IdList\"]\n",
        "\n",
        "def fetch_details(id_list):\n",
        "    ids = ','.join(id_list)\n",
        "    handle = Entrez.efetch(db=\"pubmed\", id=ids, retmode=\"xml\")\n",
        "    papers = Entrez.read(handle, validate=False)\n",
        "    handle.close()\n",
        "    time.sleep(0.4)  # To prevent overwhelming the server\n",
        "    return papers\n",
        "\n",
        "\n",
        "def save_initial_search_results(papers, filename='pubmed_initial_results.csv'):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, delimiter=';')\n",
        "        writer.writerow(['Title', 'Authors', 'Journal', 'Year', 'PMID', 'DOI', 'Abstract'])\n",
        "        for paper in papers['PubmedArticle']:\n",
        "            article = paper['MedlineCitation']['Article']\n",
        "            title = article.get('ArticleTitle', 'Not available')\n",
        "            pubmed_id = paper['MedlineCitation']['PMID']\n",
        "            journal = article['Journal'].get('Title', 'Not available')\n",
        "            year = article['Journal']['JournalIssue']['PubDate'].get('Year', 'Not available')\n",
        "            authors_list = article.get('AuthorList', [])\n",
        "            authors = ', '.join([f\"{a.get('LastName', '')}, {a.get('ForeName', '')}\" for a in authors_list])\n",
        "            abstract_text = \" \".join(article['Abstract']['AbstractText']).replace(\";\",\":\") if 'Abstract' in article else 'Not available'\n",
        "            doi = next((id for id in paper['PubmedData']['ArticleIdList'] if id.attributes.get('IdType') == 'doi'), 'Not available')\n",
        "            writer.writerow([title, authors, journal, year, pubmed_id, doi, abstract_text])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8IEdBbWxsqa"
      },
      "outputs": [],
      "source": [
        "# Complete extraction of full search\n",
        "query_result = search_pubmed(combined_search_string, retmax=10000)\n",
        "paper_details = fetch_details(query_result)\n",
        "\n",
        "# Save the initial search results\n",
        "save_initial_search_results(paper_details)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 3: Download the randomized sample and annotate it manually"
      ],
      "metadata": {
        "id": "zR4Stl6P4peX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take out randomized sample and save as csv-file:\n",
        "df = pd.read_csv('pubmed_initial_results.csv', sep=';')\n",
        "df = df.sample(n=100, random_state=48)\n",
        "\n",
        "#Save as csv file for later analysis with LLMs\n",
        "df.to_csv('100_sample.csv', index=False)\n",
        "#Save as excel file for manual annotation of sample\n",
        "df.to_excel('100_sample.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "gO0Iptff4R8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the sample, and annotate it manually before moving on to the next step. The annotations will be used to evaluate which models achieve highest diagnostic test accuracy for title and abstract screening."
      ],
      "metadata": {
        "id": "V2OWUGhKYFb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 3: LLM analysis of the random sample"
      ],
      "metadata": {
        "id": "kZNQtlVilOu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved random sample\n",
        "df = pd.read_csv('100_sample.csv', sep=',')"
      ],
      "metadata": {
        "id": "ADtAvlUpb78k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define prompt for the LLMs\n",
        "llm_prompt = 'Please assess the title and abstract based on the following criteria for inclusion in a systematic review: population: Does the study examine adults with suspected neurological disease(s)? [yes/no/NA]. intervention: Does the study use neural networks or deep learning for brain scan analysis? [yes/no/NA]. control: Does the study compare these methods against a standard reference (radiological report or expert readers)? [yes/no/NA]. outcome:  Does it measure the diagnostic accuracy of the intervention using an external cohort for validation? [yes/no/NA]. decision: Include if all the above criteria are met, exclude if any criteria are not met, uncertain if information is insufficient [include/exclude/uncertain].  Title and abstract:'\n",
        "\n",
        "\n",
        "def analyze_abstract_with_gpt35T(llm_prompt, title, abstract_text):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        temperature=0.2,\n",
        "        max_tokens=60,\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a medical researcher analyzing abstracts for a systematic review. Answer in JSON format with the following keys: population, intervention, control, outcome, decision.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{llm_prompt} {title} {abstract_text} \"}\n",
        "        ]\n",
        "    )\n",
        "    openai_response = completion.choices[0].message.content\n",
        "    return openai_response\n",
        "\n",
        "\n",
        "def analyze_abstract_with_gpt4T(llm_prompt, title, abstract_text):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo-2024-04-09\" ,\n",
        "        temperature=0.2,\n",
        "        max_tokens=60,\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a medical researcher analyzing abstracts for a systematic review. Answer in JSON format with the following keys: population, intervention, control, outcome, decision.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{llm_prompt} {title} {abstract_text} \"}\n",
        "        ]\n",
        "    )\n",
        "    openai_response = completion.choices[0].message.content\n",
        "    return openai_response\n",
        "\n",
        "def analyze_abstract_with_GPT4o(llm_prompt, title, abstract_text):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-2024-05-13\",\n",
        "        temperature=0.2,\n",
        "        max_tokens=60,\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a medical researcher analyzing abstracts for a systematic review. Answer in JSON format with the following keys: population, intervention, control, outcome, decision.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{llm_prompt} {title} {abstract_text} \"}\n",
        "        ]\n",
        "    )\n",
        "    openai_response = completion.choices[0].message.content\n",
        "    return openai_response\n",
        "\n",
        "def analyze_abstract_with_claude3_opus(llm_prompt, title, abstract_text):\n",
        "    try:\n",
        "        message = anthropic_client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            temperature=0.2,\n",
        "            max_tokens=60,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": \"You are a medical researcher analyzing abstracts for a systematic review. Answer in JSON format with the following keys: population, intervention, control, outcome, decision.\" f\"{llm_prompt} {title} {abstract_text} \"}\n",
        "            ]\n",
        "        )\n",
        "        return message.content[0].text\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Claude-3 opus API: {str(e)}\")\n",
        "        return \"API request failed\"\n",
        "\n",
        "\n",
        "def analyze_abstract_with_claude3_sonnet(llm_prompt, title, abstract_text):\n",
        "    try:\n",
        "        message = anthropic_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            temperature=0.2,\n",
        "            max_tokens=60,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": \"You are a medical researcher analyzing abstracts for a systematic review. Answer in JSON format with the following keys: population, intervention, control, outcome, decision.\" f\"{llm_prompt} {title} {abstract_text} \"}\n",
        "            ]\n",
        "        )\n",
        "        return message.content[0].text\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Claude-3 sonnet API: {str(e)}\")\n",
        "        return \"API request failed\"\n",
        "\n",
        "\n",
        "def analyze_abstract_with_claude3_haiku(llm_prompt, title, abstract_text):\n",
        "    try:\n",
        "        message = anthropic_client.messages.create(\n",
        "            model=\"claude-3-haiku-20240307\",\n",
        "            temperature=0.2,\n",
        "            max_tokens=60,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": \"You are a medical researcher analyzing abstracts for a systematic review. Answer in JSON format with the following keys: population, intervention, control, outcome, decision.\" f\"{llm_prompt} {title} {abstract_text} \"}\n",
        "            ]\n",
        "        )\n",
        "        return message.content[0].text\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Claude-3 haiku API: {str(e)}\")\n",
        "        return \"API request failed\"\n",
        "\n",
        "\n",
        "def parse_json_to_columns(json_string):\n",
        "    keys = ['population', 'intervention', 'control', 'outcome', 'decision']\n",
        "    try:\n",
        "        data = json.loads(json_string)\n",
        "        return {key: data.get(key, 'NA') for key in keys}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return {key: 'parsing_error' for key in keys}\n",
        "\n",
        "# Function to split data into batches\n",
        "def split_data_into_batches(df, batch_size=100):\n",
        "    num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
        "    for i in range(num_batches):\n",
        "        batch = df.iloc[i*batch_size:(i+1)*batch_size]\n",
        "        batch.to_csv(f'batch_{i+1}.csv', index=False)\n",
        "    return num_batches\n",
        "\n",
        "\n",
        "def analyze_selected_batches(batches, llm_prompt):\n",
        "    for batch_id in batches:\n",
        "        df = pd.read_csv(f'batch_{batch_id}.csv')\n",
        "\n",
        "        # Add columns for each model analysis\n",
        "        df['GPT35T'] = df.apply(lambda x: analyze_row_with_gpt35T(llm_prompt, x), axis=1)\n",
        "        df['gpt4T'] = df.apply(lambda x: analyze_row_with_gpt4T(llm_prompt, x), axis=1)\n",
        "        df['GPT4o'] = df.apply(lambda x: analyze_row_with_GPT4o(llm_prompt, x), axis=1)\n",
        "        df['claude3opus'] = df.apply(lambda x: analyze_row_with_claude3_opus(llm_prompt, x), axis=1)\n",
        "        df['claude3sonnet'] = df.apply(lambda x: analyze_row_with_claude3_sonnet(llm_prompt, x), axis=1)\n",
        "        df['claude3haiku'] = df.apply(lambda x: analyze_row_with_claude3_haiku(llm_prompt, x), axis=1)\n",
        "\n",
        "        # Parsing and expanding JSON results to separate columns for each model\n",
        "        gpt35_columns = df['GPT35T'].apply(parse_json_to_columns).apply(pd.Series)\n",
        "        gpt35_columns = gpt35_columns.rename(columns=lambda x: f'GPT35T_{x}')\n",
        "\n",
        "        gpt4_columns = df['gpt4T'].apply(parse_json_to_columns).apply(pd.Series)\n",
        "        gpt4_columns = gpt4_columns.rename(columns=lambda x: f'gpt4T_{x}')\n",
        "\n",
        "        GPT4o_columns = df['GPT4o'].apply(parse_json_to_columns).apply(pd.Series)\n",
        "        GPT4o_columns = GPT4o_columns.rename(columns=lambda x: f'GPT4o_{x}')\n",
        "\n",
        "        claude3_opus_columns = df['claude3opus'].apply(parse_json_to_columns).apply(pd.Series)\n",
        "        claude3_opus_columns = claude3_opus_columns.rename(columns=lambda x: f'claude3opus_{x}')\n",
        "\n",
        "        claude3_sonnet_columns = df['claude3sonnet'].apply(parse_json_to_columns).apply(pd.Series)\n",
        "        claude3_sonnet_columns = claude3_sonnet_columns.rename(columns=lambda x: f'claude3sonnet_{x}')\n",
        "\n",
        "        claude3_haiku_columns = df['claude3haiku'].apply(parse_json_to_columns).apply(pd.Series)\n",
        "        claude3_haiku_columns = claude3_haiku_columns.rename(columns=lambda x: f'claude3haiku_{x}')\n",
        "\n",
        "        # Combining all into one DataFrame\n",
        "        df_extended = pd.concat([df, gpt35_columns, gpt4_columns, GPT4o_columns, claude3_opus_columns, claude3_sonnet_columns, claude3_haiku_columns], axis=1)\n",
        "        df_extended.to_csv(f'analyzed_batch_{batch_id}.csv', index=False)\n",
        "\n",
        "def analyze_row_with_gpt35T(llm_prompt, x):\n",
        "    if x['Abstract'] != 'Not available':\n",
        "        analysis = analyze_abstract_with_gpt35T(llm_prompt, x['Title'], x['Abstract'])\n",
        "        time.sleep(1)  # Sleep to avoid rate limits\n",
        "        return analysis\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "def analyze_row_with_gpt4T(llm_prompt, x):\n",
        "    if x['Abstract'] != 'Not available':\n",
        "        analysis = analyze_abstract_with_gpt4T(llm_prompt, x['Title'], x['Abstract'])\n",
        "        time.sleep(1)  # Sleep to avoid rate limits\n",
        "        return analysis\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "def analyze_row_with_GPT4o(llm_prompt, x):\n",
        "    if x['Abstract'] != 'Not available':\n",
        "        analysis = analyze_abstract_with_GPT4o(llm_prompt, x['Title'], x['Abstract'])\n",
        "        time.sleep(1)  # Sleep to avoid rate limits\n",
        "        return analysis\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "def analyze_row_with_claude3_opus(llm_prompt, x):\n",
        "    if x['Abstract'] != 'Not available':\n",
        "        analysis = analyze_abstract_with_claude3_opus(llm_prompt, x['Title'], x['Abstract'])\n",
        "        time.sleep(1)  # Sleep to avoid rate limits\n",
        "        return analysis\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "\n",
        "def analyze_row_with_claude3_sonnet(llm_prompt, x):\n",
        "    if x['Abstract'] != 'Not available':\n",
        "        analysis = analyze_abstract_with_claude3_sonnet(llm_prompt, x['Title'], x['Abstract'])\n",
        "        time.sleep(1)  # Sleep to avoid rate limits\n",
        "        return analysis\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "\n",
        "def analyze_row_with_claude3_haiku(llm_prompt, x):\n",
        "    if x['Abstract'] != 'Not available':\n",
        "        analysis = analyze_abstract_with_claude3_haiku(llm_prompt, x['Title'], x['Abstract'])\n",
        "        time.sleep(1)  # Sleep to avoid rate limits\n",
        "        return analysis\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "\n",
        "# Splitting data into batches and analyzing them\n",
        "num_batches = split_data_into_batches(df, batch_size=100) #You can chose smaller batch sizes, for example 5, here to test if the pipeline and API works first\n",
        "print(f\"Data split into {num_batches} batches.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jvBjmVbqJegb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_batches = [1]  # You can specify multiple batches here (e.g. 1,2,3)\n",
        "analyze_selected_batches(selected_batches, llm_prompt)"
      ],
      "metadata": {
        "id": "8Ghhl4WKJexN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the analyzed batches, and conduct dataanalysis in R. You can find the analyzed csv file in the folder in the left column."
      ],
      "metadata": {
        "id": "L6nBHxlW6ibM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}